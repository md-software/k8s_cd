#
# This manifest is used for POC pipeline
#
# Create deployment for newsapi test pipeline
#
apiVersion: apps/v1
kind: Deployment
metadata:
  name: newsapi
  namespace: pipeline
spec:
  selector:
    matchLabels:
      app: newsapi
  replicas: 1
  template:
    metadata:
      labels:
        app: newsapi
    spec:
      containers:
      - name: newsapi
        image: heartysoft/alpine-confluent-kafka:python-3.6-latest
        imagePullPolicy: "IfNotPresent"
        command:
        - python3
        - /mnt/configmap/newsapi-kafka.py
        volumeMounts:
        - mountPath: /pipeline-data
          name: pipeline-data
        - name: "configmap-volume"
          mountPath: "/mnt/configmap"
      volumes:
      - name: configmap-volume
        configMap:
          name: configmap-newsapi
      - name: pipeline-data
        persistentVolumeClaim:
          claimName: spark-cephfs-pvc
      imagePullSecrets:
      - name: default-secret

---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: newsapi-spark
  namespace: pipeline
spec:
  imagePullSecrets:
    - default-secret
  type: Python
  pythonVersion: "3"
  mode: cluster

  image: "zebre8844/spark-py-kafka:v3.3.1"
  imagePullPolicy: Always
  mainApplicationFile: local:///mnt/configmap/newsapi_stream.py
  sparkVersion: "3.3.1"
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "/tmp"
    "spark.kubernetes.file.upload.path": "/tmp"
    "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp" 
    "spark.executor.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp" 
  restartPolicy:
    type: Always

  driver:
    cores: 1
    #coreLimit: "1200m"
    memory: "1g"
    labels:
      version: 3.3.1
    serviceAccount: spark
    volumeMounts:
      - name: "pipeline-data"
        mountPath: "/tmp"
      - name: "configmap-volume"
        mountPath: "/mnt/configmap"

  executor:
    cores: 1
    instances: 1
    memory: "1g"
    labels:
      version: 3.3.1
    volumeMounts:
      - name: "pipeline-data"
        mountPath: "/tmp"
      - name: "configmap-volume"
        mountPath: "/mnt/configmap"

  volumes:
    - name: configmap-volume
      configMap:
        name: configmap-newsapi
    - name: "pipeline-data"
      persistentVolumeClaim:
        claimName: spark-cephfs-pvc
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: newsapi-spark-batch
  namespace: pipeline
spec:
  schedule: "@every 10m"
  concurrencyPolicy: Allow
  successfulRunHistoryLimit: 5
  failedRunHistoryLimit: 3
  template:
    imagePullSecrets:
      - default-secret
    type: Python
    pythonVersion: "3"
    mode: cluster

    image: "zebre8844/spark-py-kafka:v3.3.1"
    imagePullPolicy: Always
    mainApplicationFile: local:///mnt/configmap/newsapi_batch.py
    sparkVersion: "3.3.1"
    sparkConf:
      "spark.eventLog.enabled": "true"
      "spark.eventLog.dir": "/tmp"
      "spark.kubernetes.file.upload.path": "/tmp"
      "spark.driver.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp" 
      "spark.executor.extraJavaOptions": "-Divy.cache.dir=/tmp -Divy.home=/tmp" 
    restartPolicy:
      type: Never

    driver:
      cores: 1
      #coreLimit: "1200m"
      memory: "1g"
      labels:
        version: 3.3.1
      serviceAccount: spark
      volumeMounts:
        - name: "pipeline-data"
          mountPath: "/tmp"
        - name: "configmap-volume"
          mountPath: "/mnt/configmap"

    executor:
      cores: 1
      instances: 1
      memory: "1g"
      labels:
        version: 3.3.1
      volumeMounts:
        - name: "pipeline-data"
          mountPath: "/tmp"
        - name: "configmap-volume"
          mountPath: "/mnt/configmap"

    volumes:
      - name: configmap-volume
        configMap:
          name: configmap-newsapi
      - name: "pipeline-data"
        persistentVolumeClaim:
          claimName: spark-cephfs-pvc
